{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030855ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sai charan\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\sai charan\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sai charan\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sai charan\\anaconda3\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in c:\\users\\sai charan\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sai charan\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\sai charan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sai charan\\anaconda3\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sai charan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\sai charan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: sklearn in c:\\users\\sai charan\\anaconda3\\lib\\site-packages (0.0.post1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sai charan\\anaconda3\\lib\\site-packages (1.24.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install sklearn\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9f413ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from math import pi\n",
    "from math import exp\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1559b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(r'C:\\Users\\Sai charan\\Documents\\Machine Learning\\emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0de1bb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Email No.'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1b8145",
   "metadata": {},
   "source": [
    "# Using inbuilt function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657d11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = data['Prediction']\n",
    "# X = data.drop(['Prediction'], axis=1)\n",
    "# X = np.array(X)\n",
    "# Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf5e4ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X.shape)\n",
    "# print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1074cbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, Y_train, Y_test=train_test_split(X, Y, test_size=0.13)\n",
    "# print(X_train.shape)\n",
    "# print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e30d30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf=MultinomialNB()\n",
    "# clf.fit(X_train,Y_train)\n",
    "# y_pred = clf.predict(X_test)\n",
    "# accuracy_score(y_pred,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15eae67",
   "metadata": {},
   "source": [
    "## Without using inbuilt function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bc08d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.46171693735499 %\n"
     ]
    }
   ],
   "source": [
    "# Utility functions \n",
    "def mean(numbers):\n",
    " return sum(numbers)/float(len(numbers))\n",
    " \n",
    "def stdev(numbers):\n",
    " avg = mean(numbers)\n",
    " variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    " return sqrt(variance)\n",
    " \n",
    "def summarize_dataset(dataset):\n",
    " summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    " del(summaries[-1])\n",
    " return summaries\n",
    "\n",
    "def calculate_gaussian_probability(x, mean, stdev):\n",
    " exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    " return (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    " \n",
    "def get_class_probabilities(summaries, row):\n",
    " total_rows = sum([summaries[label][0][2] for label in summaries])\n",
    " probabilities = dict()\n",
    " for class_value, class_summaries in summaries.items():\n",
    "     probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "     for i in range(len(class_summaries)):\n",
    "         mean, stdev, _ = class_summaries[i]\n",
    "     probabilities[class_value] *= calculate_gaussian_probability(row[i], mean, stdev)\n",
    " return probabilities\n",
    "\n",
    "############################################################################\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    " dataset_split = list()\n",
    " dataset_copy = list(dataset)\n",
    " fold_size = int(len(dataset) / n_folds)\n",
    " for _ in range(n_folds):\n",
    "     fold = list()\n",
    "     while len(fold) < fold_size:\n",
    "         index = randrange(len(dataset_copy))\n",
    "         fold.append(dataset_copy.pop(index))\n",
    "     dataset_split.append(fold)\n",
    " return dataset_split\n",
    " \n",
    "def accuracy_metric(actual, predicted):\n",
    " correct = 0\n",
    " for i in range(len(actual)):\n",
    "     if actual[i] == predicted[i]:\n",
    "         correct += 1\n",
    " return correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, n_folds):\n",
    " folds = cross_validation_split(dataset, n_folds)\n",
    " scores = list()\n",
    " # training in batches\n",
    " for fold in folds:\n",
    "     train_set = folds\n",
    "     train_set.remove(fold)\n",
    " train_set = sum(train_set, [])\n",
    " test_set = list()\n",
    " for row in fold:\n",
    "     row_copy = list(row)\n",
    "     test_set.append(row_copy)\n",
    "     row_copy[-1] = None\n",
    " predicted = naive_bayes(train_set, test_set)\n",
    " actual = [row[-1] for row in fold]\n",
    " accuracy = accuracy_metric(actual, predicted)\n",
    " scores.append(accuracy)\n",
    " return scores\n",
    " \n",
    "# Split the dataset by class values, returns a dictionary\n",
    "# 0s will contain list of non spam emails and 1 will contain list of spam emails\n",
    "def separate_by_class(dataset):\n",
    " separated = dict()\n",
    " for i in range(len(dataset)):\n",
    "     vector = dataset[i]\n",
    "     class_value = vector[-1]\n",
    "     if (class_value not in separated):\n",
    "         separated[class_value] = list()\n",
    "     separated[class_value].append(vector)\n",
    " return separated\n",
    "\n",
    " \n",
    "# Split dataset by class then calculate statistics for each row -> Getting the mean and standard deviation\n",
    "def summarize_by_class(dataset):\n",
    " separated = separate_by_class(dataset)\n",
    " summaries = dict()\n",
    " for class_value, rows in separated.items():\n",
    "     summaries[class_value] = summarize_dataset(rows)\n",
    " return summaries\n",
    " \n",
    "# Predict the class for a given row\n",
    "def predict(summaries, row):\n",
    " probabilities = get_class_probabilities(summaries, row)\n",
    " best_label, best_prob = None, -1\n",
    " for class_value, probability in probabilities.items():\n",
    "     if best_label is None or probability > best_prob:\n",
    "         best_prob = probability\n",
    "         best_label = class_value\n",
    " return best_label\n",
    " \n",
    "# Naive Bayes Algorithm\n",
    "def naive_bayes(train, test):\n",
    " summarize = summarize_by_class(train)\n",
    " predictions = list()\n",
    " for row in test:\n",
    "     output = predict(summarize, row)\n",
    "     predictions.append(output)\n",
    " return(predictions)\n",
    " \n",
    "## Starting... \n",
    "dataset = np.array(data)\n",
    "n_folds = 2\n",
    "scores = evaluate_algorithm(dataset, n_folds)\n",
    "print(f\"Accuracy: {(sum(scores)/float(len(scores)))} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408d7932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
